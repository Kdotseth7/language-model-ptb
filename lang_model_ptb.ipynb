{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34573d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necesssary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Set device = CUDA if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (/Users/kushagraseth/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2c46ab59794cc8925d06ad85e50a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the dataset using HuggingFace load_dataset\n",
    "ptb = load_dataset('ptb_text_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417c978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 42068\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3761\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3370\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# View Dataset Splits\n",
    "print('Dataset Split:', ptb)\n",
    "\n",
    "# Train Data\n",
    "train_data = ptb['train']['sentence']\n",
    "# Val Data\n",
    "val_data = ptb['validation']['sentence']\n",
    "# Test Data\n",
    "test_data = ptb['test']['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff649d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize(text):\n",
    "    return [i for i in text.split()]\n",
    "\n",
    "# Load Words\n",
    "def load_words(data):\n",
    "    tokenized_sent = list()\n",
    "    for sentence in data:\n",
    "        tokenized_sent.append(tokenize('<start> ' + sentence))\n",
    "    return sum(tokenized_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23ec1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  10000\n"
     ]
    }
   ],
   "source": [
    "# List of Tokenized Words in the Corpus\n",
    "words = load_words(train_data)\n",
    "\n",
    "# Vocab: List of Unique Words\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Print Vocab Length\n",
    "VOCAB_LEN = len(vocab)\n",
    "print('Vocab Length: ', VOCAB_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4554a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-to-Index Dictionary for Vocab\n",
    "word2idx = { term: idx for idx, term in enumerate(vocab) }\n",
    "\n",
    "# Index-to-Word Dictionary for Vocab\n",
    "idx2word = { idx: word for word,idx in word2idx.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dc1f918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words present in GloVe\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "GLOVE_DIM = 300\n",
    "glove = vocab.GloVe(name = '840B', dim = GLOVE_DIM)\n",
    "\n",
    "print('Loaded {} words present in GloVe'.format(len(glove.itos)))\n",
    "\n",
    "# Get Embedding for given word\n",
    "def get_word_embedding(word):\n",
    "    return glove.vectors[glove.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f093ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tensor = torch.zeros(1, GLOVE_DIM) # Word Embedding Tensor for <start>\n",
    "unk_tensor = torch.rand(1, GLOVE_DIM) # Word Embedding Tensor for <unk>\n",
    "\n",
    "# Create Embedding Matrix for Vocab\n",
    "embeddings = []\n",
    "for word in word2idx:\n",
    "    if word in glove.stoi: # If word present in GloVe\n",
    "        embeddings.append(get_word_embedding(word)) \n",
    "    else:\n",
    "        if(word == '<start>'): # If word is <start>\n",
    "            embeddings.append(start_tensor) \n",
    "        else: # If word is <unk> or not present in GloVe\n",
    "            embeddings.append(unk_tensor)\n",
    "            \n",
    "temp_list = []\n",
    "for emb in embeddings:\n",
    "    temp_list.append(emb.numpy().squeeze().tolist())\n",
    "# Tensor of Word Embeddings for each word in vocab\n",
    "embeddings_tensor = torch.tensor(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03117fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangModel Class for DataLoader\n",
    "class LangModelDataset(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 data: list):\n",
    "        self.data = data\n",
    "        self.sequence_length = 30\n",
    "        self.words = self.load_words()\n",
    "        self.token_list = list() # List of tokens in the Corpus\n",
    "        for word in self.words:\n",
    "            if word in word2idx:\n",
    "                self.token_list.append(word2idx[word])\n",
    "            else:\n",
    "                self.token_list.append(1)\n",
    "    \n",
    "    # Length of Number of Sequences for a Dataset split\n",
    "    def __len__(self):\n",
    "        return len(self.token_list) - self.sequence_length\n",
    "    \n",
    "    # List of Tokenized Words in the Corpus\n",
    "    def load_words(self):\n",
    "        tokenized_sent = list()\n",
    "        for sentence in self.data:\n",
    "            tokenized_sent.append(self.tokenize('<start> ' + sentence))\n",
    "        return sum(tokenized_sent, [])\n",
    "    \n",
    "    def __getitem__(self, \n",
    "                    idx: int):\n",
    "        x = torch.tensor(self.token_list[idx : idx + self.sequence_length])\n",
    "        y = torch.tensor(self.token_list[idx + 1 : idx + self.sequence_length + 1])\n",
    "        return x, y\n",
    "        \n",
    "    # Tokenize a sentence using split()  \n",
    "    def tokenize(self, \n",
    "                 text: str):\n",
    "        return [i for i in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "303e4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([44, 45, 46,  0, 47, 26, 27, 28, 29, 48, 49, 41, 42, 50, 51, 52, 53, 54,\n",
      "        55, 35, 36, 37, 42, 56, 57, 58, 59,  0, 35, 60]), tensor([45, 46,  0, 47, 26, 27, 28, 29, 48, 49, 41, 42, 50, 51, 52, 53, 54, 55,\n",
      "        35, 36, 37, 42, 56, 57, 58, 59,  0, 35, 60, 42]))\n"
     ]
    }
   ],
   "source": [
    "# Language Model Object for DataLoader\n",
    "# Train Dataset Object\n",
    "train_ds = LangModelDataset(train_data)\n",
    "\n",
    "# Val Dataset Object\n",
    "val_ds = LangModelDataset(val_data)\n",
    "\n",
    "# Test Dataset Object\n",
    "test_ds = LangModelDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88f95442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyper-Parameters\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = GLOVE_DIM\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = VOCAB_LEN\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = False\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9be8a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  64, 2043,  133,  ...,   26,  108, 3394],\n",
      "        [ 495,   64, 3098,  ...,   35, 1319,  189],\n",
      "        [  27, 2821,  229,  ...,  918, 1607, 1608],\n",
      "        ...,\n",
      "        [ 103,   42,  465,  ...,  160,   26,  119],\n",
      "        [  98,  935,  392,  ...,   32,  663,  790],\n",
      "        [6755,   95,  108,  ..., 1258, 6689, 2742]]), tensor([[2043,  133,   40,  ...,  108, 3394, 3395],\n",
      "        [  64, 3098,    0,  ..., 1319,  189, 2479],\n",
      "        [2821,  229, 7681,  ..., 1607, 1608, 1656],\n",
      "        ...,\n",
      "        [  42,  465,   40,  ...,   26,  119, 2590],\n",
      "        [ 935,  392,  336,  ...,  663,  790, 9406],\n",
      "        [  95,  108,   35,  ..., 6689, 2742,   30]])]\n"
     ]
    }
   ],
   "source": [
    "# Pytorch Data Loaders\n",
    "# Train Data Loader\n",
    "train_loader = DataLoader(train_ds, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          shuffle = True)\n",
    "# Val Data Loader\n",
    "val_loader = DataLoader(val_ds, \n",
    "                        batch_size = BATCH_SIZE, \n",
    "                        shuffle = True)\n",
    "# Test Data Loader\n",
    "test_loader = DataLoader(test_ds, \n",
    "                        batch_size = 1, \n",
    "                        shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05018441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Training Batch\n",
    "iterator = iter(train_loader)\n",
    "inputs = next(iterator)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6e92e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN / LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        # Initialize Embedding Layer with Pre-Trained GloVe Embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        # Initialzie LSTM layer to process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True)\n",
    "#         # Initialzie RNN layer to process the vector sequences \n",
    "#         self.rnn = nn.RNN(embedding_dim, \n",
    "#                           hidden_dim, \n",
    "#                           num_layers = n_layers, \n",
    "#                           bidirectional = bidirectional, \n",
    "#                           dropout = dropout, \n",
    "#                           batch_first = True, \n",
    "#                           nonlinearity = 'relu')\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        # Initialize Dense layers to predict\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_directions, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # Initialize dropout to improve with regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Forward Pass of Model    \n",
    "    def forward(self, \n",
    "                x):\n",
    "        # Embedding Layer\n",
    "        embedded = self.embedding(x)\n",
    "        # Dropout Layer before LSTM Layer\n",
    "        embedded = self.dropout(embedded)\n",
    "        # LSTM Layer\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "#         # RNN Layer\n",
    "#         output, hidden = self.rnn(embedded)\n",
    "        # 1st Fully Connected Layer\n",
    "        output = self.fc1(output)\n",
    "        # Dropout Layer before Output\n",
    "        output = self.dropout(output)\n",
    "        # 2nd Fully Connected Layer\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17ad18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model:  LSTM(\n",
      "  (embedding): Embedding(10000, 300)\n",
      "  (rnn): RNN(300, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set Seed Value to make results reproducible\n",
    "torch.manual_seed(32)\n",
    "\n",
    "model = LSTM(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            NUM_LAYERS, \n",
    "            BIDIRECTION, \n",
    "            DROPOUT).to(device)\n",
    "\n",
    "print('LSTM Model: ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f887dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Function\n",
    "def train(loader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x) # [Batch_Length, Sequence Length, Output Dim]\n",
    "        \n",
    "        # Convert y_pred to 2D Tensor\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]) # [Batch_Length * Sequence Length, Output Dim]\n",
    "        # Convert y to 1D Tensor\n",
    "        y = torch.flatten(y) # [Batch_Length * Sequence Length]\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Calculate gradients for w/b\n",
    "        loss.backward()  \n",
    "        # Update weights according to optimizer rules\n",
    "        optimizer.step()\n",
    "    return round((sum(losses) / len(losses)), 4) # Return Average Loss\n",
    "\n",
    "# Model Evaluate Function\n",
    "def evaluate(loader, \n",
    "             model, \n",
    "             loss_fn):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x) # [Batch_Length, Sequence Length, Output Dim]\n",
    "              \n",
    "        # Convert y_pred to 2D Tensor\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]) # [Batch_Length * Sequence Length, Output Dim]\n",
    "        # Convert y to 1D Tensor\n",
    "        y = torch.flatten(y) # [Batch_Length * Sequence Length]\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return round((sum(losses) / len(losses)), 4) # Return Average Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4f300ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 14525/14525 [18:22<00:00, 13.17it/s, Loss=5.35]\n",
      "100%|████████████████████████████| 1153/1153 [00:32<00:00, 35.06it/s, Loss=5.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --> Train Loss: 5.2862 | Train PPL: 197.59115600585938 | Val Loss: 5.3696 | Val PPL: 214.7769012451172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training on Train dataset and Evaluation on Validation dataset\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                              lr = LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "n_epochs = 1\n",
    "PATH = f'best-model.pt'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Model Training\n",
    "    train_loss = train(train_loader, \n",
    "                       model, \n",
    "                       optimizer, \n",
    "                       loss_fn)\n",
    "    train_loss_list.append(train_loss)\n",
    "    # Train Perplexity\n",
    "    train_ppl = torch.exp(torch.tensor(train_loss))\n",
    "    \n",
    "    # Model Evaluation\n",
    "    val_loss = evaluate(val_loader, \n",
    "                        model, \n",
    "                        loss_fn)\n",
    "    val_loss_list.append(val_loss)\n",
    "    # Val Perplexity\n",
    "    val_ppl = torch.exp(torch.tensor(val_loss))\n",
    "    \n",
    "    print(\"Epoch {0} --> Train Loss: {1} | Train PPL: {2} | Val Loss: {3} | Val PPL: {4}\".format(epoch + 1, train_loss, train_ppl, val_loss, val_ppl))\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f9a8659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(10000, 300)\n",
       "  (rnn): RNN(300, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10000, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "saved_model = LSTM(EMBEDDING_DIM, \n",
    "                   HIDDEN_DIM, \n",
    "                   OUTPUT_DIM, \n",
    "                   NUM_LAYERS, \n",
    "                   BIDIRECTION, \n",
    "                   DROPOUT).to(device)\n",
    "\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f7c39094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 82400/82400 [04:06<00:00, 334.12it/s, Loss=4.86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Loss: 5.2833 | Predict PPL: 197.01895141601562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Predict Function\n",
    "def predict(loader, \n",
    "            model, \n",
    "            loss_fn):\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        with torch.no_grad():\n",
    "            # Calculate y_pred\n",
    "            y_pred = model.forward(x) # [Batch_Length, Sequence Length, Output Dim]\n",
    "\n",
    "            # Convert y_pred to 2D Tensor\n",
    "            y_pred = y_pred.view(-1, y_pred.shape[-1]) # [Batch_Length * Sequence Length, Output Dim]\n",
    "            # Convert y to 1D Tensor\n",
    "            y = torch.flatten(y) # [Batch_Length * Sequence Length]\n",
    "            \n",
    "            # Loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return round((sum(losses) / len(losses)), 4) # Return Average Loss\n",
    "\n",
    "# Model Predict\n",
    "predict_loss = predict(test_loader, \n",
    "                       saved_model, \n",
    "                       loss_fn)\n",
    "# Predict Perplexity\n",
    "predict_ppl = torch.exp(torch.tensor(predict_loss))\n",
    "\n",
    "print(\"Predict Loss: {0} | Predict PPL: {1}\".format(predict_loss, predict_ppl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
