{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34573d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Set device = CUDA if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (/Users/kushagraseth/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2c46ab59794cc8925d06ad85e50a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the dataset using HuggingFace load_dataset\n",
    "ptb = load_dataset('ptb_text_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417c978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 42068\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3761\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence'],\n",
      "        num_rows: 3370\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset Split\n",
    "print('Dataset Split:', ptb)\n",
    "\n",
    "train_data = ptb['train']['sentence']\n",
    "val_data = ptb['validation']['sentence']\n",
    "test_data = ptb['test']['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff649d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize(text):\n",
    "    return [i for i in text.split()]\n",
    "\n",
    "# Load Words\n",
    "def load_words(data):\n",
    "    tokenized_sent = list()\n",
    "    for sentence in data:\n",
    "        tokenized_sent.append(tokenize('<start> ' + sentence))\n",
    "    return sum(tokenized_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23ec1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  10000\n"
     ]
    }
   ],
   "source": [
    "# List of Tokenized Words in the Corpus\n",
    "words = load_words(train_data)\n",
    "\n",
    "# Vocab: List of Unique Words\n",
    "vocab = Counter(words)\n",
    "VOCAB_LEN = len(vocab)\n",
    "print('Vocab Length: ', VOCAB_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4554a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for Vocab\n",
    "word2idx = { term: idx for idx, term in enumerate(vocab) }\n",
    "\n",
    "idx2word = { idx: word for word,idx in word2idx.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1f918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196017 words\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe Embeddings\n",
    "GLOVE_DIM = 300\n",
    "glove = vocab.GloVe(name = '840B', dim = GLOVE_DIM)\n",
    "\n",
    "print('Loaded {} words'.format(len(glove.itos)))\n",
    "\n",
    "# Get Embedding for given word\n",
    "def get_word_embedding(word):\n",
    "    return glove.vectors[glove.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f093ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Matrix for Vocab\n",
    "start_tensor = torch.zeros(1, GLOVE_DIM) # Word Embedding for <start>\n",
    "unk_tensor = torch.rand(1, GLOVE_DIM) # Word Embedding for <unk>\n",
    "\n",
    "embeddings = []\n",
    "for word in word2idx:\n",
    "    if word in glove.stoi:\n",
    "        embeddings.append(get_word_embedding(word))\n",
    "    else:\n",
    "        if(word == '<start>'):\n",
    "            embeddings.append(start_tensor)\n",
    "        else:\n",
    "            embeddings.append(unk_tensor)\n",
    "            \n",
    "temp_list = []\n",
    "for emb in embeddings:\n",
    "    temp_list.append(emb.numpy().squeeze().tolist())\n",
    "embeddings_tensor = torch.tensor(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03117fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangModel Class for DataLoader\n",
    "class LangModelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: list):\n",
    "        self.data = data\n",
    "        self.sequence_length = 30\n",
    "        self.words = self.load_words()\n",
    "        self.token_list = list() # List of Word ID's in the Corpus\n",
    "        for word in self.words:\n",
    "            if word in word2idx:\n",
    "                self.token_list.append(word2idx[word])\n",
    "            else:\n",
    "                self.token_list.append(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_list) - self.sequence_length\n",
    "    \n",
    "    # List of Tokenized Words in the Corpus\n",
    "    def load_words(self):\n",
    "        tokenized_sent = list()\n",
    "        for sentence in self.data:\n",
    "            tokenized_sent.append(self.tokenize('<start> ' + sentence))\n",
    "        return sum(tokenized_sent, [])\n",
    "    \n",
    "    def __getitem__(self, \n",
    "                    idx: int):\n",
    "        x = torch.tensor(self.token_list[idx : idx + self.sequence_length])\n",
    "        y = torch.tensor(self.token_list[idx + 1 : idx + self.sequence_length + 1])\n",
    "        return x, y\n",
    "        \n",
    "    # Tokenize    \n",
    "    def tokenize(self, \n",
    "                 text: str):\n",
    "        return [i for i in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "303e4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([44, 45, 46,  0, 47, 26, 27, 28, 29, 48, 49, 41, 42, 50, 51, 52, 53, 54,\n",
      "        55, 35, 36, 37, 42, 56, 57, 58, 59,  0, 35, 60]), tensor([45, 46,  0, 47, 26, 27, 28, 29, 48, 49, 41, 42, 50, 51, 52, 53, 54, 55,\n",
      "        35, 36, 37, 42, 56, 57, 58, 59,  0, 35, 60, 42]))\n"
     ]
    }
   ],
   "source": [
    "# Language Model Object for DataLoader\n",
    "train_ds = LangModelDataset(train_data)\n",
    "print(train_ds[50])\n",
    "\n",
    "val_ds = LangModelDataset(val_data)\n",
    "\n",
    "test_ds = LangModelDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88f95442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyper-parameters\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = GLOVE_DIM\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = VOCAB_LEN\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = False\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9be8a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  32, 1649, 4260,  ...,   46,   99,   98],\n",
      "        [1367, 3465,    0,  ..., 3748,   32,  127],\n",
      "        [   0,   32, 6710,  ...,   27,   48,   27],\n",
      "        ...,\n",
      "        [   0,  315,  307,  ..., 3269,  160, 7670],\n",
      "        [ 938,    0, 4550,  ..., 4386,  467,  133],\n",
      "        [ 315,  374,  874,  ..., 1295,   35, 3854]]), tensor([[1649, 4260, 2668,  ...,   99,   98, 1868],\n",
      "        [3465,    0,  108,  ...,   32,  127,   40],\n",
      "        [  32, 6710,  203,  ...,   48,   27,  152],\n",
      "        ...,\n",
      "        [ 315,  307, 3257,  ...,  160, 7670, 7671],\n",
      "        [   0, 4550,   69,  ...,  467,  133,  247],\n",
      "        [ 374,  874,  251,  ...,   35, 3854,  181]])]\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Data Loaders\n",
    "train_loader = DataLoader(train_ds, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          shuffle = True)\n",
    "val_loader = DataLoader(val_ds, \n",
    "                        batch_size = BATCH_SIZE, \n",
    "                        shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(test_ds, \n",
    "                        batch_size = 1, \n",
    "                        shuffle = False)\n",
    "\n",
    "# Print Training Batch\n",
    "iterator = iter(train_loader)\n",
    "inputs = next(iterator)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e92e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        # Initialize Embedding Layer with Pre-Trained Embeddings (Vector Sequences)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        # Initialzie LSTM layer to process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        # Initialize Dense layers to predict\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_directions, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # Initialize dropout to improve with regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                x):\n",
    "        # Embedding Layer\n",
    "        embedded = self.embedding(x)\n",
    "        # Dropout Layer before LSTM Layer\n",
    "        embedded = self.dropout(embedded)\n",
    "        # LSTM Layer\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # 1st Fully Connected Layer\n",
    "        output = self.fc1(output)\n",
    "        # Dropout Layer before Output\n",
    "        output = self.dropout(output)\n",
    "        # 2nd Fully Connected Layer\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17ad18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model:  LSTM(\n",
      "  (embedding): Embedding(10000, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(32)\n",
    "\n",
    "model = LSTM(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            NUM_LAYERS, \n",
    "            BIDIRECTION, \n",
    "            DROPOUT).to(device)\n",
    "\n",
    "print('LSTM Model: ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f887dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Function\n",
    "def train(loader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Convert y_pred to 2D Tensor\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "        # Convert y to 1D Tensor\n",
    "        y = torch.flatten(y)\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Calculate gradients for w/b\n",
    "        loss.backward()  \n",
    "        # Update weights according to optimizer rules\n",
    "        optimizer.step()          \n",
    "    return round((sum(losses) / len(losses)), 4)\n",
    "\n",
    "# Model Evaluate Function\n",
    "def evaluate(loader, \n",
    "             model, \n",
    "             loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        # Calculate y_pred\n",
    "        y_pred = model(x)\n",
    "              \n",
    "        # Convert y_pred to 2D Tensor\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "        # Convert y to 1D Tensor\n",
    "        y = torch.flatten(y)\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return round((sum(losses) / len(losses)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f300ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 14525/14525 [33:26<00:00,  7.24it/s, Loss=4.17]\n",
      "100%|████████████████████████████| 1153/1153 [01:04<00:00, 17.76it/s, Loss=4.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --> Train Loss: 4.0741 | Train PPL: 58.79753875732422 | Val Loss: 4.8028 | Val PPL: 121.85114288330078\n"
     ]
    }
   ],
   "source": [
    "# Model Training on Train dataset and Evaluation on Validation dataset\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr = LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "n_epochs = 1\n",
    "PATH = f'best-model.pt'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Model Training\n",
    "    train_loss = train(train_loader, \n",
    "                       model, \n",
    "                       optimizer, \n",
    "                       loss_fn)\n",
    "    train_loss_list.append(train_loss)\n",
    "    # Train Perplexity\n",
    "    train_ppl = torch.exp(torch.tensor(train_loss))\n",
    "    \n",
    "    # Model Evaluation\n",
    "    val_loss = evaluate(val_loader, \n",
    "                        model, \n",
    "                        loss_fn)\n",
    "    val_loss_list.append(val_loss)\n",
    "    # Val Perplexity\n",
    "    val_ppl = torch.exp(torch.tensor(val_loss))\n",
    "    \n",
    "    print(\"Epoch {0} --> Train Loss: {1} | Train PPL: {2} | Val Loss: {3} | Val PPL: {4}\".format(epoch + 1, train_loss, train_ppl, val_loss, val_ppl))\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "127c4376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(10000, 300)\n",
       "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10000, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "saved_model = LSTM(EMBEDDING_DIM, \n",
    "                   HIDDEN_DIM, \n",
    "                   OUTPUT_DIM, \n",
    "                   NUM_LAYERS, \n",
    "                   BIDIRECTION, \n",
    "                   DROPOUT).to(device)\n",
    "\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d82a4e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 82400/82400 [06:05<00:00, 225.54it/s, Loss=4.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Loss: 4.7386 | Predict PPL: 114.27407836914062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Predict Function\n",
    "def predict(loader, \n",
    "            model, \n",
    "            loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        with torch.no_grad():\n",
    "            # Calculate y_pred\n",
    "            y_pred = model.forward(x)\n",
    "\n",
    "            # Convert y_pred to 2D Tensor\n",
    "            y_pred = y_pred.view(-1, y_pred.shape[-1])\n",
    "            # Convert y to 1D Tensor\n",
    "            y = torch.flatten(y)\n",
    "            \n",
    "            # Loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return round((sum(losses) / len(losses)), 4)\n",
    "\n",
    "# Model Predict\n",
    "predict_loss = predict(test_loader, \n",
    "                       saved_model, \n",
    "                       loss_fn)\n",
    "# Predict Perplexity\n",
    "predict_ppl = torch.exp(torch.tensor(predict_loss))\n",
    "\n",
    "print(\"Predict Loss: {0} | Predict PPL: {1}\".format(predict_loss, predict_ppl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
